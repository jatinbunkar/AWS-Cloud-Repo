# Triggering a Batch Process using Amazon SQS

## Description

Most of the Batch Processes on AWS can be handled using AWS Batch. This is an alternative to the AWS Batch Service. In order to organize the huge number of processes, a Queue is used to streamline the flow of processes
for better consumption. In this lab exercise, one such process (uploading a file to S3) is processed by Lambda via SQS.

### **Objective**
Create a batch processing system where a file uploaded to an S3 bucket sends a message to an SQS queue, which is then processed by a Lambda function.


---

### **Architecture Diagram**

The diagram below displays a visual representation of the application architecture:

<img width="479" alt="image" src="https://github.com/user-attachments/assets/e5d2a141-6c60-44f7-a76d-a742690c3db5">


### **Steps to Implement**

### 1. **Create an S3 Bucket**
- Open the AWS Management Console and navigate to the **S3** service.
- Click on **Create bucket**.
- Provide a **unique name** for the bucket and choose a region.
- Keep other settings as default or adjust as needed.
- Click **Create bucket**.


<img width="1180" alt="image" src="https://github.com/user-attachments/assets/08b30f96-13ad-4dc4-82ee-3a829e5a7197">


---

### 2. **Create an SQS Queue**
- Go to the **Amazon SQS** service in the AWS Console.
- Click on **Create queue**.
- Choose **Standard queue**.
- Enter a name for the queue.
- Configure other settings if needed and click **Create queue**.

<img width="1055" alt="image" src="https://github.com/user-attachments/assets/0b4bd121-50eb-4d6e-a3fe-c2bd612421b1">

<img width="1408" alt="image" src="https://github.com/user-attachments/assets/9fb04b0f-ba8e-4e90-b822-234cfb6f278f">



---

### 3. **Edit the Access Policy of SQS Queue**
- Go to your created SQS queue and select it.
- Click on the **Permissions** tab.
- Choose **Edit** under the Access Policy section.
- Copy and paste the following policy, replacing `<YOUR-BUCKET-NAME>`, `<AWS-ACCOUNT-ID>`, and `<region>`:


```json
{
  "Version": "2012-10-17",
  "Id": "PolicyForS3ToSQS",
  "Statement": [
    {
      "Sid": "AllowS3ToSendMessage",
      "Effect": "Allow",
      "Principal": {
        "Service": "s3.amazonaws.com"
      },
      "Action": "sqs:SendMessage",
      "Resource": "arn:aws:sqs:<region>:<AWS-ACCOUNT-ID>:<Queue-Name>",
      "Condition": {
        "ArnLike": {
          "aws:SourceArn": "arn:aws:s3:::<YOUR-BUCKET-NAME>"
        }
      }
    }
  ]
}

```

<img width="1009" alt="image" src="https://github.com/user-attachments/assets/f5fdba97-0c6e-43ad-85da-956062dcf8b2">


  -  Click Save

### 4. **Configure S3 Event Notification**
-  Navigate to your S3 bucket.
-  Go to the Properties tab.
-  Scroll to Event notifications and click Create event notification.
-  Fill in the following details:
    -  Name: Provide a name for the event.
    -  Event types: Select All object create events.
    -  Destination: Choose SQS Queue and enter the ARN of your SQS queue.
    -  Click Save.
 
<img width="1155" alt="image" src="https://github.com/user-attachments/assets/6d222d5c-ea0c-4ec3-8607-364c487d299a">

<img width="667" alt="image" src="https://github.com/user-attachments/assets/5ef0f83d-0692-476e-978f-b120a01c7497">
 
<img width="611" alt="image" src="https://github.com/user-attachments/assets/7c61459b-99f3-4977-9dff-b0f0d06d8c80">


### 5. **Upload an Object to S3**
-  Go to the S3 bucket.
-  Click Upload and add a file.
-  Click Upload to confirm.
-  Check the SQS queue to verify a message has been received.

<img width="676" alt="image" src="https://github.com/user-attachments/assets/7bd495dc-7204-4f5b-af11-cd4cdaa1872d">

<img width="1333" alt="image" src="https://github.com/user-attachments/assets/58c5debe-5887-447d-8ba3-fb477295010b">



### 6. **Create a Lambda Function**
-  Navigate to the AWS Lambda service.
-  Click on Create function.
-  Choose Author from scratch and fill in:
    -  Function name: Provide a name.
    -  Runtime: Select Python 3.8.
    -  Role: Choose Use an existing role and select CCL-Lambda-Role.
 
IAM ROLE:

-  **AWSLambdaSQSQueueExecutionRole**

```yaml
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "sqs:ReceiveMessage",
                "sqs:DeleteMessage",
                "sqs:GetQueueAttributes",
                "logs:CreateLogGroup",
                "logs:CreateLogStream",
                "logs:PutLogEvents"
            ],
            "Resource": "*"
        }
    ]
}
```


-  Click Create function.

<img width="671" alt="image" src="https://github.com/user-attachments/assets/fce76d92-9407-4ce8-ae86-da0c02aea37d">


### 7. **Add Code to the Lambda Function**
-  Copy and paste the following Python code:

```yaml
import json

def lambda_handler(event, context):
    for record in event['Records']:
        message = record['body']
        print(f"Received message: {message}")
    return {
        'statusCode': 200,
        'body': json.dumps('Message processed successfully')
    }

```

-  Click Deploy to save the code.

<img width="1235" alt="image" src="https://github.com/user-attachments/assets/99247b15-258a-41b3-9a81-93000ee1ec8e">

### 8. **Configure Lambda Trigger from SQS**
-  Go to the SQS queue you created.
-  Under Lambda triggers, click Configure Lambda Function trigger.
-  Select your Lambda function and click Add.
-  Ensure the status changes to Enabled.

<img width="1341" alt="image" src="https://github.com/user-attachments/assets/217ff754-65dc-40c8-a0bb-ba312ee17584">

<img width="1053" alt="image" src="https://github.com/user-attachments/assets/757f0f49-2033-4abd-b1d2-e88369a1e2ff">

<img width="1333" alt="image" src="https://github.com/user-attachments/assets/c2cb988e-e538-43b6-aeb8-587b044ed9fa">


### 9. **Test the Setup**
-  Upload a few more files to the S3 bucket.
-  Navigate to CloudWatch to monitor the logs:
-  Go to CloudWatch Logs and select the log group for your Lambda function.
-  Check the logs to view the processed file names.

<img width="1337" alt="image" src="https://github.com/user-attachments/assets/820dd4ea-7fd3-455a-af10-ebe0e17248d9">



### 10. **Output**
-  Log entries in CloudWatch should show the file names of the objects uploaded to the S3 bucket, confirming the process is working.


<img width="1203" alt="image" src="https://github.com/user-attachments/assets/8fb7f516-f7f1-4b9f-b20f-cee070295cf8">

Log: 

```yaml

Received message: {
    "Records": [
        {
            "eventVersion": "2.1",
            "eventSource": "aws:s3",
            "awsRegion": "us-east-1",
            "eventTime": "2024-11-02T20:11:25.803Z",
            "eventName": "ObjectCreated:Put",
            "userIdentity": {
                "principalId": "AWS:AIDAXJ627XJE7LUNJGZXZ"
            },
            "requestParameters": {
                "sourceIPAddress": "152.59.27.58"
            },
            "responseElements": {
                "x-amz-request-id": "CV2PZDBY3VKH48EH",
                "x-amz-id-2": "WjcQstBr0LJx/x/Ja97vbz0XFTnvno/cdISHw59J77xMkvefkGuBwpi3yLlYq0uu1d5oTl+Fym3bNvpFNuSU+/8ZceNsuJ2P5CJs8kq+CmQ="
            },
            "s3": {
                "s3SchemaVersion": "1.0",
                "configurationId": "jatins3bucketnotification",
                "bucket": {
                    "name": "jatinbucketlab",
                    "ownerIdentity": {
                        "principalId": "A2Q3ZKMRK5QDGJ"
                    },
                    "arn": "arn:aws:s3:::jatinbucketlab"
                },
                "object": {
                    "key": "13.png",
                    "size": 121515,
                    "eTag": "1bb71aa4ddf2b97a763bb51ab0035751",
                    "sequencer": "006726876DB0D44EFC"
                }
            }
        }
    ]
}

```





















